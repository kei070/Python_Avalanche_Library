#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A few helper functions for loading the features.
"""

# imports
import numpy as np
from .Balance_Data import balance_data
from .DatetimeSimple import date_dt


"""
A helper-function that just returns the input.
"""

def ret(inp):
    return inp
# end def


# extract a season
def extract_sea(all_df, sel_feats="all", drop=None, split=[2021, 2023], balance_meth="SMOTE", target_n="y",
                k_neighbors=5, n_jobs=-1, rv_target=True, ord_vars=["ftc", "ava_clim"],
                ord_lims={"ftc":[0, 1], "ava_clim":[1, 3]}):

    """
    Parameters:

        sel_feats     List of features to select. Or "all", which uses all available features.
        drop          List (or name) of feature(s) to drop. Defaults to None.
        split         Integer or float. Provide e.g. a list of years (e.g., [2021, 2023]) which will be extracted as
                                        test data.
        balance_meth  String. Set the method of balancing to be used. Choices are the following:
                              -None: no balancing
                              -undersample: [DOES NOT WORK ANYMORE] uses the custom undersample function
                              -SMOTE: uses the synthetic minority oversampling method from the imbalanced-learn library
                                      (default)
                              -SVMSMOTE: same as SMOTE but using an SVM algorithm to detect sample to use for generating
                                         new synthetic samples.
                              -KMeansSMOTE: Same as SMOTE but applies a KMeans clustering before to over-sample using
                                            SMOTE.
                              -ADASYN: uses the adaptive synthetic sampling method from the imbalanced-learn library
                              -ros: uses the random oversampling method from the imbalanced-learn library
                              -rus: uses the random undersampling method from the imbalanced-learn library
        target_n      String. The name of the target variable.
        k_neighbors   Integer. The number of neighbouring values SMOTE or ADASYN use to generate synthetic values.
                               Defaults to 5 and is only used if SMOTE or ADASYN is used as balancing method.
        n_jobs        Integer. Number of CPU cores used during the cross-validation loop. Defaults to -1, meaning all
                               all available cores will be used. Only used for SMOTE and ADASYN.
        rv_target     Logical. If True (default) it is made sure that the target variable as defined by target_n is
                               removed from the training data.
        ord_vars      List. A list of the names of variables that are on an ordinal scale. Since, e.g., SMOTE does not
                            natively support ordinary variables, the variables are rounded to integers after being
                            generated by SMOTE. Defaults to ["ftc", "ava_clim"]
        ord_lims      Dict. A dictionary of length-2 lists with the limits of the ordinal scale of the respective
                            ordinal variable.
    """

    # check the drop parameter and drop the requested variables
    if type(drop) != type(None):
        all_df.drop(drop, inplace=True, axis=1, errors="ignore")
    # end if

    # treat the sel_feats = "all" case
    if type(sel_feats) == str:
        if sel_feats == "all":
            sel_feats = slice(None)
        # end if
    # end if

    # make sure split is a list so that iteration is possible
    if type(split) == int:
        split = [split]
    # end if

    test_all_inds = []

    for sp in split:

        # make sure sp is an integer
        sp = int(sp)

        # extract the data for the requested avalanche season
        test_all_inds.append((all_df.index >= date_dt(sp-1, 7, 1)) & (all_df.index < date_dt(sp, 7, 1)))

    # end for sp

    test_all_inds = np.logical_or.reduce(test_all_inds)

    test_all_df = all_df[test_all_inds]
    train_all_df = all_df[~test_all_inds]

    train_x_all = train_all_df[sel_feats]
    train_y_all = train_all_df[target_n]
    test_x_all = test_all_df[sel_feats]
    test_y_all = test_all_df[target_n]

    # remove the target variable from the training data
    if rv_target:
        train_x_all.drop(target_n, axis=1, inplace=True, errors="ignore")
        test_x_all.drop(target_n, axis=1, inplace=True, errors="ignore")
    # end if

    # return train_x_all, test_x_all, train_y_all, test_y_all

    # perform the balancing if requested
    if str(balance_meth) != "None":
        train_x, train_y = balance_data(train_x_all, train_y_all, method=balance_meth, k_neighbors=k_neighbors,
                                        n_jobs=n_jobs)
        test_x, test_y = balance_data(test_x_all, test_y_all, method=balance_meth, k_neighbors=k_neighbors,
                                      n_jobs=n_jobs)

        for ord_var in ord_vars:
            try:
                train_x[ord_var] = train_x[ord_var].round().clip(ord_lims[ord_var][0], ord_lims[ord_var][1])
                test_x[ord_var] = test_x[ord_var].round().clip(ord_lims[ord_var][0], ord_lims[ord_var][1])
            except:
                print(f"\n{ord_var} not in features. Continuing...\n")
                continue
        # end for ord_var

        return train_x, test_x, train_y, test_y, train_x_all, test_x_all, train_y_all, test_y_all
    else:
        return train_x_all, test_x_all, train_y_all, test_y_all
    # end if else
# end def


# extract a region
def extract_reg(all_df, sel_feats, split, balance_meth, k_neighbors=5, target_n="y", n_jobs=-1, rv_target=True):

    """
    See extract_sea for documentation.
    """

    # make sure split is a list so that iteration is possible
    if type(split) == int:
        split = [split]
    # end if

    test_all_inds = []

    for sp in split:

        # make sure sp is an integer
        sp = int(sp)

        # extract the data for the requested region
        test_all_inds.append(all_df["reg_code"] == sp)

    # end for sp

    test_all_inds = np.logical_or.reduce(test_all_inds)

    test_all_df = all_df[test_all_inds]
    train_all_df = all_df[~test_all_inds]

    # extract x and y data
    train_x_all = train_all_df[sel_feats]
    train_y_all = train_all_df[target_n]
    test_x_all = test_all_df[sel_feats]
    test_y_all = test_all_df[target_n]

    # remove the target variable from the training data
    if rv_target:
        train_x_all.drop(target_n, axis=1, inplace=True)
        test_x_all.drop(target_n, axis=1, inplace=True)
    # end if

    # perform the balancing if requested
    if str(balance_meth) != "None":
        train_x, train_y = balance_data(train_x_all, train_y_all, method=balance_meth, k_neighbors=k_neighbors,
                                        n_jobs=n_jobs)
        test_x, test_y = balance_data(test_x_all, test_y_all, method=balance_meth, k_neighbors=k_neighbors,
                                      n_jobs=n_jobs)

        return train_x, test_x, train_y, test_y, train_x_all, test_x_all, train_y_all, test_y_all
    else:
        return train_x_all, test_x_all, train_y_all, test_y_all
    # end if else

# end def